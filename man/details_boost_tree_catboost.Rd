% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/boost_tree_catboost.R
\name{details_boost_tree_catboost}
\alias{details_boost_tree_catboost}
\title{Boosted trees via catboost}
\description{
\code{catboost::catboost.train()} creates a series of decision trees
forming an ensemble. Each tree depends on the results of previous trees.
All trees in the ensemble are combined to produce a final prediction.
catboost has native support for categorical predictors.
}
\details{
For this engine, there are multiple modes: regression and classification
\subsection{Tuning Parameters}{

This model has 7 tuning parameters:
\itemize{
\item \code{tree_depth}: Tree Depth (type: integer, default: 6L)
\item \code{trees}: # Trees (type: integer, default: 1000L)
\item \code{learn_rate}: Learning Rate (type: double, default: 0.03)
\item \code{mtry}: Proportion Randomly Selected Predictors (type: double,
default: see below)
\item \code{min_n}: Minimal Node Size (type: integer, default: 1L)
\item \code{sample_size}: Proportion Observations Sampled (type: double, default:
see below)
\item \code{stop_iter}: # Iterations Before Stopping (type: integer, default:
Inf)
}

The \code{mtry} parameter controls the proportion of predictors that will be
randomly sampled at each split. catboost’s \code{rsm} argument natively
expects a proportion between 0 and 1. The default is to use all
predictors (\code{rsm = 1}).

Unlike lightgbm and xgboost, bonsai does not currently convert \code{mtry}
from a count to a proportion for catboost. Users should set
\code{counts = FALSE} in \code{set_engine()} and supply \code{mtry} as a proportion
directly. For example, \code{mtry = 0.5} with \code{counts = FALSE} means 50\% of
predictors are considered at each split.
\subsection{Engine-Specific Parameters}{
\itemize{
\item \code{max_leaves}: Maximum number of leaves in each tree (only used when
the grow policy is \code{Lossguide}).
\item \code{l2_leaf_reg}: L2 regularization coefficient for leaf values (default:
3.0).
}
}

}

\subsection{Translation from parsnip to the original package (regression)}{

The \strong{bonsai} extension package is required to fit this model.

\if{html}{\out{<div class="sourceCode r">}}\preformatted{boost_tree(
  mtry = integer(), trees = integer(), min_n = integer(), tree_depth = integer(),
  learn_rate = numeric(), sample_size = numeric(), stop_iter = integer()
) |>
  set_engine("catboost") |>
  set_mode("regression") |>
  translate()
}\if{html}{\out{</div>}}

\if{html}{\out{<div class="sourceCode">}}\preformatted{## Boosted Tree Model Specification (regression)
## 
## Main Arguments:
##   mtry = integer()
##   trees = integer()
##   min_n = integer()
##   tree_depth = integer()
##   learn_rate = numeric()
##   sample_size = numeric()
##   stop_iter = integer()
## 
## Computational engine: catboost 
## 
## Model fit template:
## bonsai::train_catboost(x = missing_arg(), y = missing_arg(), 
##     weights = missing_arg(), rsm = integer(), iterations = integer(), 
##     min_data_in_leaf = integer(), depth = integer(), learning_rate = numeric(), 
##     subsample = numeric(), early_stopping_rounds = integer(), 
##     thread_count = 1, allow_writing_files = FALSE, random_seed = sample.int(10^5, 
##         1))
}\if{html}{\out{</div>}}
}

\subsection{Translation from parsnip to the original package (classification)}{

The \strong{bonsai} extension package is required to fit this model.

\if{html}{\out{<div class="sourceCode r">}}\preformatted{boost_tree(
  mtry = integer(), trees = integer(), min_n = integer(), tree_depth = integer(),
  learn_rate = numeric(), sample_size = numeric(), stop_iter = integer()
) |>
  set_engine("catboost") |>
  set_mode("classification") |>
  translate()
}\if{html}{\out{</div>}}

\if{html}{\out{<div class="sourceCode">}}\preformatted{## Boosted Tree Model Specification (classification)
## 
## Main Arguments:
##   mtry = integer()
##   trees = integer()
##   min_n = integer()
##   tree_depth = integer()
##   learn_rate = numeric()
##   sample_size = numeric()
##   stop_iter = integer()
## 
## Computational engine: catboost 
## 
## Model fit template:
## bonsai::train_catboost(x = missing_arg(), y = missing_arg(), 
##     weights = missing_arg(), rsm = integer(), iterations = integer(), 
##     min_data_in_leaf = integer(), depth = integer(), learning_rate = numeric(), 
##     subsample = numeric(), early_stopping_rounds = integer(), 
##     thread_count = 1, allow_writing_files = FALSE, random_seed = sample.int(10^5, 
##         1))
}\if{html}{\out{</div>}}

\code{\link[bonsai:train_catboost]{bonsai::train_catboost()}} is a wrapper
around \code{catboost::catboost.train()} (and other functions) that makes it
easier to run this model.
}

\subsection{Preprocessing requirements}{

This engine does not require any special encoding of the predictors.
Categorical predictors can be partitioned into groups of factor levels
(e.g. \verb{\{a, c\}} vs \verb{\{b, d\}}) when splitting at a node. Dummy variables
are not required for this model.

Unlike many other boosting engines, catboost has native support for
categorical predictors. When a factor predictor is passed to the model,
catboost will compute target-based statistics to create numeric features
from the factor levels. This often provides better performance than
using dummy variables.

Non-numeric predictors (i.e., factors) are internally converted to
numeric using catboost’s native categorical feature handling. In the
classification context, non-numeric outcomes (i.e., factors) are also
internally converted to numeric.
}

\subsection{Case weights}{

This model can utilize case weights during model fitting. To use them,
see the documentation in \link{case_weights} and the examples
on \code{tidymodels.org}.

The \code{fit()} and \code{fit_xy()} arguments have arguments called
\code{case_weights} that expect vectors of case weights.
}

\subsection{Prediction types}{

\if{html}{\out{<div class="sourceCode r">}}\preformatted{parsnip:::get_from_env("boost_tree_predict") |>
  dplyr::filter(engine == "catboost") |>
  dplyr::select(mode, type)
}\if{html}{\out{</div>}}

\if{html}{\out{<div class="sourceCode">}}\preformatted{## # A tibble: 4 x 2
##   mode           type   
##   <chr>          <chr>  
## 1 regression     numeric
## 2 classification class  
## 3 classification prob   
## 4 classification raw
}\if{html}{\out{</div>}}
}

\subsection{Other details}{
\subsection{Bagging}{

The \code{sample_size} argument is translated to the \code{subsample} parameter in
catboost. The argument is interpreted by catboost as a \emph{proportion}
rather than a count, so bonsai internally reparameterizes the
\code{sample_size} argument with
\code{\link[dials:trees]{dials::sample_prop()}} during tuning.

The default value for \code{subsample} depends on the dataset size and the
bootstrap type. For datasets with fewer than 100 observations, no
sampling is performed (equivalent to \code{sample_size = 1}). For larger
datasets, the default is 0.66 for Poisson or Bernoulli bootstrap and 0.8
for MVS bootstrap.
}

\subsection{Verbosity}{

bonsai quiets much of the logging output from
\code{catboost::catboost.train()} by default. With default settings, logged
warnings and errors will still be passed on to the user. To print out
all logs during training, set \code{quiet = FALSE}.
}

}

\subsection{Saving fitted model objects}{

This model object contains data that are not required to make
predictions. When saving the model for the purpose of prediction, the
size of the saved object might be substantially reduced by using
functions from the \href{https://butcher.tidymodels.org}{butcher} package.
}

\subsection{Examples}{

The “Introduction to bonsai” article contains
\href{https://bonsai.tidymodels.org/articles/bonsai.html}{examples} of
\code{boost_tree()} with the \code{"catboost"} engine.
}

\subsection{References}{
\itemize{
\item \href{https://arxiv.org/abs/1706.09516}{CatBoost: unbiased boosting with categorical features}
\item \href{https://catboost.ai/}{CatBoost: gradient boosting with categorical features support}
}
}
}
\keyword{internal}
