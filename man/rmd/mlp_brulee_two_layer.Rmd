```{r}
#| child: aaa.Rmd
#| include: false
```

`r descr_models("mlp", "brulee_two_layer")`

## Tuning Parameters

```{r}
#| label: brulee-param-info
#| echo: false
defaults <- 
  tibble::tibble(parsnip = c("hidden_units", "penalty", "dropout", "epochs", "learn_rate", "activation", "mixture"),
                 default = c("3L", "0.001", "0.0", "100L", "0.01", "'relu'", "0.0"))

spec <-
  mlp(
    hidden_units = tune(),
    penalty = tune(),
    dropout = tune(),
    epochs = tune(),
    learn_rate = tune(),
    activation = tune()
  ) |> 
  set_engine("brulee_two_layer")

param <- spec |> make_parameter_list(defaults)
```

This model has `r nrow(param)` tuning parameters:

```{r}
#| label: brulee-param-list
#| echo: false
#| results: asis
param$item
```

The use of the L1 penalty (a.k.a. the lasso penalty) does _not_ force parameters to be strictly zero (as it does in packages such as glmnet). The zeroing out of parameters is a specific feature the optimization method used in those packages.

Both `penalty` and `dropout` should be not be used in the same model. 

Other engine arguments of interest: 

 - `hidden_layer_2` and `activation_2` control the format of the second layer. 
 - `momentum`: A number used to use historical gradient information during optimization.
 - `batch_size`: An integer for the number of training set points in each batch.
 - `class_weights`: Numeric class weights. See [brulee::brulee_mlp()].
 - `stop_iter`: A non-negative integer for how many iterations with no improvement before stopping. (default: 5L).
 - `rate_schedule`: A function to change the learning rate over epochs. See [brulee::schedule_decay_time()] for details. 


## Translation from parsnip to the original package (regression)

```{r}
#| label: brulee-reg
mlp(
  hidden_units = integer(1),
  penalty = double(1),
  dropout = double(1),
  epochs = integer(1),
  learn_rate = double(1),
  activation = character(1)
) |>
  set_engine("brulee_two_layer",
             hidden_units_2 = integer(1),
             activation_2 = character(1)) |> 
  set_mode("regression") |> 
  translate()
```

Note that parsnip automatically sets the linear activation in the last layer. 

## Translation from parsnip to the original package (classification)

```{r}
#| label: brulee-cls
mlp(
  hidden_units = integer(1),
  penalty = double(1),
  dropout = double(1),
  epochs = integer(1),
  learn_rate = double(1),
  activation = character(1)
) |> 
  set_engine("brulee_two_layer",
             hidden_units_2 = integer(1),
             activation_2 = character(1)) |> 
  set_mode("classification") |> 
  translate()
```


## Preprocessing requirements

```{r}
#| child: template-makes-dummies.Rmd
```

```{r}
#| child: template-same-scale.Rmd
```

## Case weights

```{r}
#| child: template-no-case-weights.Rmd
```

## References

 - Kuhn, M, and K Johnson. 2013. _Applied Predictive Modeling_. Springer.



