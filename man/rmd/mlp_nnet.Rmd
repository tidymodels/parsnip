```{r, child = "setup.Rmd", include = FALSE}
```

`r descr_models("mlp", "nnet")`

## Tuning Parameters

```{r nnet-param-info, echo = FALSE}
defaults <- 
  tibble::tibble(parsnip = c("hidden_units", "penalty", "epochs"),
                 default = c("none", "0.0", "100L"))

param <-
  mlp() %>% 
  set_engine("nnet") %>% 
  tunable() %>% 
  dplyr::select(-source, -component, -component_id, parsnip = name) %>% 
  dplyr::mutate(
    dials = purrr::map(call_info, get_dials),
    label = purrr::map_chr(dials, ~ .x$label),
    type = purrr::map_chr(dials, ~ .x$type)
  ) %>% 
  dplyr::full_join(defaults, by = "parsnip") %>% 
  mutate(
    item = 
      glue::glue("- `{parsnip}`: {label} (type: {type}, default: {default})\n\n")
  )
```

This model has `r nrow(param)` tuning parameters:

```{r nnet-param-list, echo = FALSE, results = "asis"}
param$item
```

Note that, in [nnet::nnet()], the maximum number of parameters is an argument with a fairly low value of `maxit = 1000`. For some models, you may need to pass this value in via [set_engine()] so that the model does not fail. 


## Translation from parsnip to the original package (regression)

```{r nnet-reg}
mlp(
  hidden_units = integer(1),
  penalty = double(1),
  epochs = integer(1)
) %>%  
  set_engine("nnet") %>% 
  set_mode("regression") %>% 
  translate()
```

Note that \pkg{parsnip} automatically sets linear activation in the last layer. 

## Translation from parsnip to the original package (classification)

```{r nnet-cls}
mlp(
  hidden_units = integer(1),
  penalty = double(1),
  epochs = integer(1)
) %>% 
  set_engine("nnet") %>% 
  set_mode("classification") %>% 
  translate()
```


## Preprocessing requirements

```{r child = "template-makes-dummies.Rmd"}
```

```{r child = "template-same-scale.Rmd"}
```

## References

 - Kuhn, M, and K Johnson. 2013. _Applied Predictive Modeling_. Springer.



