```{r, child = "aaa.Rmd", include = FALSE}
```

`r descr_models("boost_tree", "lightgbm")`

## Tuning Parameters

```{r lightgbm-param-info, echo = FALSE}
defaults <- 
  tibble::tibble(parsnip = c("mtry", "trees", "tree_depth", "learn_rate", "min_n",  "loss_reduction"),
                 default = c("see below", 100L, -1, 0.1, 20, 0))

# For this model, this is the same for all modes
param <-
 boost_tree() %>% 
  set_engine("lightgbm") %>% 
  set_mode("regression") %>% 
  make_parameter_list(defaults)
```

This model has `r nrow(param)` tuning parameters:

```{r lightgbm-param-list, echo = FALSE, results = "asis"}
param$item
```

The `mtry` parameter gives the _number_ of predictors that will be randomly sampled at each split. The default is to use all predictors. 

Rather than as a number, [lightgbm::lgb.train()]'s `feature_fraction` argument encodes `mtry` as the _proportion_ of predictors that will be randomly sampled at each split. parsnip translates `mtry`, supplied as the _number_ of predictors, to a proportion under the hood. That is, the user should still supply the argument as `mtry` to `boost_tree()`, and do so in its sense as a number rather than a proportion; before passing `mtry` to [lightgbm::lgb.train()], parsnip will convert the `mtry` value to a proportion. 

Note that parsnip's translation can be overridden via the `counts` argument, supplied to `set_engine()`. By default, `counts` is set to `TRUE`, but supplying the argument `counts = FALSE` allows the user to supply `mtry` as a proportion rather than a number.

## Translation from parsnip to the original package (regression)

`r uses_extension("boost_tree", "lightgbm", "regression")`

```{r lightgbm-reg}
boost_tree(
  mtry = integer(), trees = integer(), tree_depth = integer(), 
  learn_rate = numeric(), min_n = integer(), loss_reduction = numeric()
) %>%
  set_engine("lightgbm") %>%
  set_mode("regression") %>%
  translate()
```

## Translation from parsnip to the original package (classification)

`r uses_extension("boost_tree", "lightgbm", "classification")`

```{r lightgbm-cls}
boost_tree(
  mtry = integer(), trees = integer(), tree_depth = integer(), 
  learn_rate = numeric(), min_n = integer(), loss_reduction = numeric()
) %>% 
  set_engine("lightgbm") %>% 
  set_mode("classification") %>% 
  translate()
```

[train_lightgbm()] is a wrapper around [lightgbm::lgb.train()] (and other functions) that make it easier to run this model. 

## Other details

### Preprocessing

```{r child = "template-tree-split-factors.Rmd"}
```

Non-numeric predictors (i.e., factors) are internally converted to numeric. In the classification context, non-numeric outcomes (i.e., factors) are also internally converted to numeric. 

### Verbosity

bonsai quiets much of the logging output from [lightgbm::lgb.train()] by default. With default settings, logged warnings and errors will still be passed on to the user. To print out all logs during training, set `quiet = TRUE`.

## Examples 

<!-- TODO: update url to bonsai pkgdown site -->
The "Introduction to bonsai" article contains [examples](https://github.com/tidymodels/bonsai) of `boost_tree()` with the `"lightgbm"` engine.

## References

 - [LightGBM: A Highly Efficient Gradient Boosting Decision Tree](https://papers.nips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html)
