```{r}
#| child: aaa.Rmd
#| include: false
```

`r descr_models("mlp", "h2o")`

## Tuning Parameters

```{r}
#| label: h2o-param-info
#| echo: false
defaults <- 
  tibble::tibble(parsnip = c("hidden_units", "penalty", "dropout", "epochs", "learn_rate", "activation"),
                 default = c("200L", "0.0", "0.5", "10", "0.005", "'see below'"))

param <-
  mlp() |> 
  set_engine("h2o") |> 
  make_parameter_list(defaults)
```

This model has `r nrow(param)` tuning parameters:

```{r}
#| label: h2o-param-list
#| echo: false
#| results: asis
param$item
```

The naming of activation functions in [h2o::h2o.deeplearning()] differs from parsnip's conventions. Currently, only "relu" and "tanh" are supported and will be converted internally to "Rectifier" and "Tanh" passed to the fitting function.

`penalty` corresponds to l2 penalty. [h2o::h2o.deeplearning()] also supports specifying the l1 penalty directly with the engine argument `l1`. 

Other engine arguments of interest: 

- `stopping_rounds` controls early stopping rounds based on the convergence of another engine parameter `stopping_metric`. By default, [h2o::h2o.deeplearning] stops training if simple moving average of length 5 of the stopping_metric does not improve for 5 scoring events.  This is mostly useful when used alongside the engine parameter `validation`, which is the __proportion__ of train-validation split, parsnip will split and pass the two data frames to h2o. Then [h2o::h2o.deeplearning] will evaluate the metric and early stopping criteria on the validation set. 

- h2o uses a 50% dropout ratio controlled by `dropout` for hidden layers by default. [h2o::h2o.deeplearning()] provides an engine argument `input_dropout_ratio` for dropout ratios in the input layer, which defaults to 0. 


## Translation from parsnip to the original package (regression)


[agua::h2o_train_mlp] is a wrapper around [h2o::h2o.deeplearning()].

```{r}
#| label: h2o-reg
mlp(
  hidden_units = integer(1),
  penalty = double(1),
  dropout = double(1),
  epochs = integer(1),
  learn_rate = double(1),
  activation = character(1)
) |>  
  set_engine("h2o") |> 
  set_mode("regression") |> 
  translate()
```

## Translation from parsnip to the original package (classification)

```{r}
#| label: h2o-cls
mlp(
  hidden_units = integer(1),
  penalty = double(1),
  dropout = double(1),
  epochs = integer(1),
  learn_rate = double(1),
  activation = character(1)
) |> 
  set_engine("h2o") |> 
  set_mode("classification") |> 
  translate()
```


## Preprocessing requirements

```{r}
#| child: template-makes-dummies.Rmd
```

```{r}
#| child: template-same-scale.Rmd
```


By default, [h2o::h2o.deeplearning()] uses the argument `standardize = TRUE` to center and scale all numeric columns. 


## Initializing h2o 

```{r}
#| child: template-h2o-init.Rmd
```

## Saving fitted model objects

```{r}
#| child: template-bundle.Rmd
```
