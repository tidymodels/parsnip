```{r, child = "setup.Rmd", include = FALSE}
```

`r descr_models("boost_tree", "xgboost")`

## Tuning Parameters

```{r xgboost-param-info, echo = FALSE}
defaults <- 
  tibble::tibble(parsnip = c("tree_depth", "trees", "learn_rate", "mtry", "min_n", "loss_reduction", "sample_size", "stop_iter"),
                 default = c("6L", "15L", "0.3", "see below", "1L", "0.0", "1.0", "Inf"))

# For this model, this is the same for all modes
param <-
 boost_tree() %>% 
  set_engine("xgboost") %>% 
  set_mode("regression") %>% 
  tunable() %>% 
  dplyr::select(-source, -component, -component_id, parsnip = name) %>% 
  dplyr::mutate(
    dials = purrr::map(call_info, get_dials),
    label = purrr::map_chr(dials, ~ .x$label),
    type = purrr::map_chr(dials, ~ .x$type)
  ) %>% 
  dplyr::full_join(defaults, by = "parsnip") %>% 
  mutate(
    item = 
      glue::glue("- `{parsnip}`: {label} (type: {type}, default: {default})\n\n")
  )
```

This model has `r nrow(param)` tuning parameters:

```{r xgboost-param-list, echo = FALSE, results = "asis"}
param$item
```

The `mtry` parameter is related to the number of predictors. The default is to use all predictors. [xgboost::xgb.train()] encodes this as a real number between zero and one. `parsnip` translates the number of columns to this type of value. The user should give the argument to `boost_tree()` an integer (not a real number). 

## Translation from parsnip to the original package (regression)

```{r xgboost-reg}
boost_tree(
  mtry = integer(), trees = integer(), min_n = integer(), tree_depth = integer(),
  learn_rate = numeric(), loss_reduction = numeric(), sample_size = numeric(),
  stop_iter = integer()
) %>%
  set_engine("xgboost") %>%
  set_mode("regression") %>%
  translate()
```

## Translation from parsnip to the original package (classification)

```{r xgboost-cls}
boost_tree(
  mtry = integer(), trees = integer(), min_n = integer(), tree_depth = integer(),
  learn_rate = numeric(), loss_reduction = numeric(), sample_size = numeric(),
  stop_iter = integer()
) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification") %>% 
  translate()
```

[xgb_train()] is a wrapper around [xgboost::xgb.train()] (and other functions) that makes it easier to run this model. 

## Preprocessing requirements

`xgboost` does not have a means to translate factor predictors to grouped splits; it requires that non-numeric predictors (e.g., factors) must be converted to dummy variables or some other numeric representation. By default, when using [fit()] with `xgboost`, a one-hot encoding is used to convert factor predictors to indicator variables.

## Other details

### Sparse matrices

`xgboost` requires the data to be in a spares format. If your predictor data are already in this format, then use [fit_xy()] to pass it to the model function. Otherwise, `parsnip` converts the data to this format. 

### Parallel processing

By default, the model is trained without parallel processing. This can be change by passing the `nthread` parameter to [set_engine()]. However, it is unwise to combine this with external parallel processing when using the `tune` package. 

### Early stopping

The `stop_iter()`  argument allows the model to prematurely stop training if the objective function does not improve within `early_stop` iterations. 

The best way to use this feature is in conjunction with an _internal validation set_. To do this, pass the `validation` parameter of [xgb_train()] via the `parsnip` [set_engine()] function. This is the proportion of the training set that should be reserved for measuring performance (and stop early). 

If the model specification has `early_stop >= trees`, `early_stop` is converted to `trees - 1` and a warning is issued. 

### Objective function

`parsnip` chooses the objective function based on the characteristics of the outcome. To use a different loss, pass the `objective` argument to [set_engine()]. 

## References

 - [XGBoost: A Scalable Tree Boosting System](https://arxiv.org/abs/1603.02754)
 
 - Kuhn, M, and K Johnson. 2013. _Applied Predictive Modeling_. Springer.
