```{r, child = "setup.Rmd", include = FALSE}
```

`r descr_models("bart", "dbarts")`

## Tuning Parameters

```{r bart-param-info, echo = FALSE}
defaults <- 
  tibble::tibble(parsnip = c("trees", "prior_terminal_node_coef",
                             "prior_terminal_node_expo", "prior_outcome_range"),
                 default = c("200L", "0.95", "2.00", "2.00")
  )

param <-
 bart() %>% 
  set_engine("dbarts") %>% 
  set_mode("regression") %>% 
  tunable() %>% 
  dplyr::select(-source, -component, -component_id, parsnip = name) %>% 
  dplyr::mutate(
    dials = purrr::map(call_info, get_dials),
    label = purrr::map_chr(dials, ~ .x$label),
    type = purrr::map_chr(dials, ~ .x$type)
  ) %>% 
  dplyr::full_join(defaults, by = "parsnip") %>% 
  mutate(
    item = 
      glue::glue("- `{parsnip}`: {label} (type: {type}, default: {default})\n\n")
  )
```

This model has `r nrow(param)` tuning parameters:

```{r bart-param-list, echo = FALSE, results = "asis"}
param$item
```

## Important engine-specific options

Some relevant arguments that can be passed to `set_engine()`: 

* `keepevery`, `n.thin`:	Every `keepevery` draw is kept to be returned to the user. Useful for "thinning" samples.

* `ntree`, `n.trees`: The number of trees in the sum-of-trees formulation.

* `ndpost`, `n.samples`: The number of posterior draws after burn in, `ndpost` / `keepevery` will actually be returned.

* `nskip`, `n.burn`: Number of MCMC iterations to be treated as burn in.

* `nchain`, `n.chains`: Integer specifying how many independent tree sets and fits should be calculated.

* `nthread`, `n.threads`: Integer specifying how many threads to use. Depending on the CPU architecture, using more than the number of chains can degrade performance for small/medium data sets. As such some calculations may be executed single threaded regardless.

* `combinechains`, `combineChains`: Logical; if `TRUE`, samples will be returned in arrays of dimensions equal to `nchain` times `ndpost` times number of observations.

## Translation from parsnip to the original package (classification)

```{r bart-cls}
bart(
  trees = integer(1),
  prior_terminal_node_coef = double(1),
  prior_terminal_node_expo = double(1),
  prior_outcome_range = double(1)
) %>% 
  set_engine("dbarts") %>% 
  set_mode("classification") %>% 
  translate()
```


## Translation from parsnip to the original package (regression)

```{r bart-reg}
bart(
  trees = integer(1),
  prior_terminal_node_coef = double(1),
  prior_terminal_node_expo = double(1),
  prior_outcome_range = double(1)
) %>% 
  set_engine("dbarts") %>% 
  set_mode("regression") %>% 
  translate()
```

## Preprocessing requirements

```{r child = "template-makes-dummies.Rmd"} 
```

[dbarts::bart()] will also convert the factors to indicators if the user does not create them first. 


## References

 - Chipman, George, McCulloch. "BART: Bayesian additive regression trees." _Ann. Appl. Stat._ 4 (1) 266 - 298, March 2010.
