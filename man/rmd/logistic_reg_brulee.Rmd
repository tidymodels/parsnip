```{r, child = "aaa.Rmd", include = FALSE}
```

`r descr_models("logistic_reg", "brulee")`

## Tuning Parameters

```{r brulee-param-info, echo = FALSE}
defaults <- 
  tibble::tibble(parsnip = c("penalty", "mixture"),
                 default = c(  "0.001", "0.0"))

param <-
  logistic_reg() %>% 
  set_engine("brulee") %>% 
  make_parameter_list(defaults)
```

This model has `r nrow(param)` tuning parameter:

```{r brulee-param-list, echo = FALSE, results = "asis"}
param$item
```

The use of the L1 penalty (a.k.a. the lasso penalty) does _not_ force parameters to be strictly zero (as it does in packages such as glmnet). The zeroing out of parameters is a specific feature the optimization method used in those packages.

Other engine arguments of interest: 

 - `optimizer()`: The optimization method. See [brulee::brulee_linear_reg()].
 - `epochs()`: An integer for the number of passes through the training set. 
 - `lean_rate()`: A number used to accelerate the gradient decsent process. 
 - `momentum()`: A number used to use historical gradient information during optimization  (`optimizer = "SGD"` only).
 - `batch_size()`: An integer for the number of training set points in each batch.
 - `stop_iter()`: A non-negative integer for how many iterations with no improvement before stopping. (default: 5L).
 - `class_weights()`: Numeric class weights. See [brulee::brulee_logistic_reg()].


## Translation from parsnip to the original package (classification)

```{r brulee-cls}
logistic_reg(penalty = double(1)) %>% 
  set_engine("brulee") %>% 
  translate()
```


```{r child = "template-makes-dummies.Rmd"}
```

```{r child = "template-same-scale.Rmd"}
```

## Case weights

```{r child = "template-no-case-weights.Rmd"}
```

## References

 - Kuhn, M, and K Johnson. 2013. _Applied Predictive Modeling_. Springer.
