```{r, child = "aaa.Rmd", include = FALSE}
```

`r descr_models("linear_reg", "gls")`

## Tuning Parameters

This model has no tuning parameters.

## Translation from parsnip to the original package

`r uses_extension("linear_reg", "gls", "regression")`

```{r gls-csl}
library(multilevelmod)

linear_reg() %>% 
  set_engine("gls") %>% 
  set_mode("regression") %>% 
  translate()
```


## Preprocessing requirements

There are no specific preprocessing needs. However, it is helpful to keep the clustering/subject identifier column as factor or character (instead of making them into dummy variables). See the examples in the next section. 

## Other details

The model can accept case weights. 

With parsnip, we suggest using the _fixed effects_ formula method when fitting, but the details of the correlation structure should be passed to `set_engine()` since it is an irregular (but required) argument:

```{r}
library(tidymodels)
# load nlme to be able to use the `cor*()` functions
library(nlme)

data("riesby")

linear_reg() %>% 
  set_engine("gls", correlation =  corCompSymm(form = ~ 1 | subject)) %>% 
  fit(depr_score ~ week, data = riesby)
```

When using tidymodels infrastructure, it may be better to use a workflow. In this case, you can add the appropriate columns using `add_variables()` then supply the typical formula when adding the model: 

```r
library(tidymodels)

gls_spec <- 
  linear_reg() %>% 
  set_engine("gls", correlation =  corCompSymm(form = ~ 1 | subject))

gls_wflow <- 
  workflow() %>% 
  # The data are included as-is using:
  add_variables(outcomes = depr_score, predictors = c(week, subject)) %>% 
  add_model(gls_spec, formula = depr_score ~ week)

fit(gls_wflow, data = riesby)
```

# Degrees of freedom

Note that [nlme::lme()] and [nlme::gls()] can fit the same model but will count degrees of freedom differently. If there are `n` data points, `p` fixed effects parameters, and `q` random effect parameters, the residual degrees of freedom are:

* **lme**: n - p - q
* **gls**: n - p

As a result, p-values will be different. For example, we can fit the same model using different estimation methods (assuming a positive covariance value): 

```{r}
gls_fit <- 
  linear_reg() %>% 
  set_engine("gls", correlation =  corCompSymm(form = ~ 1 | subject)) %>% 
  fit(depr_score ~ week, data = riesby)

lme_fit <-
  linear_reg() %>% 
  set_engine("lme", random =  ~ 1 | subject) %>% 
  fit(depr_score ~ week, data = riesby)
```

The estimated within-subject correlations are the same:

```{r}
library(ape)

# lme, use ape package:
lme_within_sub <- varcomp(lme_fit$fit)/sum(varcomp(lme_fit$fit))
lme_within_sub["subject"]

# gls:
summary(gls_fit$fit$modelStruct)
```

as are the fixed effects (and their standard errors):

```{r}
nlme::fixef(lme_fit$fit)
coef(gls_fit$fit)
```

However, the p-values for the fixed effects are different:

```{r, include = FALSE}
library(broom.mixed)
```
```{r}
library(broom.mixed)

# lme:
lme_fit %>% tidy() %>% filter(group == "fixed") %>% select(-group, -effect)

# gls:
gls_fit %>% tidy()
```



## References

- J Pinheiro, and D Bates. 2000. _Mixed-effects models in S and S-PLUS_. Springer, New York, NY
 
