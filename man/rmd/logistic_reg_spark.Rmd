```{r, child = "setup.Rmd", include = FALSE}
```

`r descr_models("logistic_reg", "spark")`

## Tuning Parameters

```{r spark-param-info, echo = FALSE}
defaults <- 
  tibble::tibble(parsnip = c("penalty", "mixture"),
                 default = c("0.0", "0.0"))

param <-
  logistic_reg() %>% 
  set_engine("spark") %>% 
  tunable() %>% 
  dplyr::select(-source, -component, -component_id, parsnip = name) %>% 
  dplyr::mutate(
    dials = purrr::map(call_info, get_dials),
    label = purrr::map_chr(dials, ~ .x$label),
    type = purrr::map_chr(dials, ~ .x$type)
  ) %>% 
  dplyr::full_join(defaults, by = "parsnip") %>% 
  dplyr::mutate(
    item = 
      glue::glue("- `{parsnip}`: {label} (type: {type}, default: {default})\n\n")
  )
```

This model has `r nrow(param)` tuning parameters:

```{r spark-param-list, echo = FALSE, results = "asis"}
param$item
```

For `penalty`, the amount of regularization includes both the L1 penalty (i.e., lasso) and the L2 penalty (i.e., ridge or weight decay). 

A value of `mixture = 1` corresponds to a pure lasso model, while `mixture = 0` indicates ridge regression.

## Translation from parsnip to the original package

```{r spark-csl}
logistic_reg(penalty = double(1), mixture = double(1)) %>% 
  set_engine("spark") %>% 
  translate()
```

## Preprocessing requirements

```{r child = "template-makes-dummies.Rmd"}
```

```{r child = "template-same-scale.Rmd"}
```
By default, `ml_logistic_regression()` uses the argument `standardization = TRUE` to center and scale the data. 

## Other details

```{r child = "template-spark-notes.Rmd"}
```

## References

 - Luraschi, J, K Kuo, and E Ruiz. 2019. _Mastering Spark with R_. O'Reilly Media
 
 - Hastie, T, R Tibshirani, and M Wainwright. 2015. _Statistical Learning with Sparsity_. CRC Press.
 
 - Kuhn, M, and K Johnson. 2013. _Applied Predictive Modeling_. Springer.

